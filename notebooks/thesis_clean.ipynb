{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df06ed3e",
   "metadata": {},
   "source": [
    "# Online Correlation Changepoint Detection via Adaptive EWMA Charts\n",
    "\n",
    "This notebook investigates **real-time detection of shifts in correlation** between two time series. The goal is to detect a correlation changepoint as early as possible using only past data.\n",
    "\n",
    "To do so, we propose to monitor the **rolling Pearson correlation** between paired data streams based on a fixed rolling window. We then propose the **Adaptive EWMA** chart, which updates its EWMA forgetting factor based on one-step prediction error. We evaluate and compare it against three other online changepoint detection methods:\n",
    "\n",
    "- **CUSUM** – classical mean-shift detector applied to rolling correlation  \n",
    "- **Wilcoxon Rank-Sum Scan** – non-parametric, windowed scan method  \n",
    "- **MEWMA** – multivariate EWMA on the bivariate raw stream  \n",
    "\n",
    "This work is part of a Master's thesis:\n",
    "\n",
    "> **Online Correlation Changepoint Detection via Adaptive EWMA Charts**  \n",
    "> *Jilly Lim — Imperial College London, 2025*\n",
    "\n",
    "The notebook includes:\n",
    "- Controlled simulations and calibration of each method  \n",
    "- Real-world case studies:  \n",
    "  - Stock prices (e.g. NVDA vs GOOGL, Zoom vs S&P 500)  \n",
    "  - COVID-19 cases (Los Angeles vs San Francisco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88190dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import ranksums\n",
    "import itertools\n",
    "\n",
    "try:\n",
    "    import kagglehub\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please `pip install kagglehub` to fetch datasets used in the case studies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03592b8",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation & Rolling Correlation\n",
    "\n",
    "## Generation Functions\n",
    "\n",
    "First we generate synthetic Gaussian datastreams $(X_t, Y_t)$ for our simulation study outlined in Section 4.1 of the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bivariate_stream(rho0=0.20, v=50, G=180, D=300, delta=0.7, mode='test', seed=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic data stream (X_t, Y_t) with a jump in correlation at time tau.\n",
    "\n",
    "    Parameters:\n",
    "    - rho0: baseline correlation before the jump\n",
    "    - v: lambda parameter for Poisson jitter xi\n",
    "    - G: grace period before the jump\n",
    "    - D: dwell period after the jump\n",
    "    - delta: jump size\n",
    "    - mode: 'test' for streams with jumps, 'control' for no-jump streams\n",
    "    - seed: optional RNG seed\n",
    "\n",
    "    Returns: \n",
    "    - df: columns ['X', 'Y'] of observations\n",
    "    - tau: the changepoint\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if not -1 < rho0 < 1:\n",
    "        raise ValueError(\"rho0 must be between -1 and 1\")\n",
    "\n",
    "    # Randomize jump location via poisson (lambda=v) RV.\n",
    "    xi = np.random.poisson(lam=v)\n",
    "    tau = G + xi\n",
    "    delta = delta if mode=='test' else 0.0\n",
    "    theta = np.random.choice([1, -1]) if mode=='test' else 0\n",
    "    jump = theta * delta\n",
    "\n",
    "    # Generate observations.\n",
    "    total_length = tau + D\n",
    "    X = np.zeros(total_length)\n",
    "    Y = np.zeros(total_length)\n",
    "    cov0 = [[1, rho0], [rho0, 1]]\n",
    "    rho1 = rho0 + jump\n",
    "    rho1 = max(min(rho1, 0.9999), -0.9999)  # cap rho1 to (-1, 1)\n",
    "    cov1 = [[1, rho1], [rho1, 1]]\n",
    "    pre = np.random.multivariate_normal(mean=[0,0], cov=cov0, size=tau)\n",
    "    post = np.random.multivariate_normal(mean=[0,0], cov=cov1, size=D)\n",
    "    X[:tau], Y[:tau] = pre[:,0], pre[:,1]\n",
    "    X[tau:], Y[tau:] = post[:,0], post[:,1]\n",
    "    df = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "    return df, tau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285e28b",
   "metadata": {},
   "source": [
    "We can visualize a single sample bivariate stream. At a first glance, it is hard to tell when the relationship between the two variables change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample bivariate stream.\n",
    "\n",
    "df, tau = generate_bivariate_stream(rho0=0.2, seed=227)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(df.index + 1, df['X'], linewidth=1, label='X_t')\n",
    "plt.plot(df.index + 1, df['Y'],  linewidth=1, label='Y_t')\n",
    "plt.axvline(tau, linestyle='--', color='red', label='Changepoint')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sample Bivariate Stream with Changepoint')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ed869",
   "metadata": {},
   "source": [
    "Based on these bivariate streams, we then generate the rolling Pearson correlation $r_t$ based on a fixed window size, which is described in detail in Section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ced87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_corr(df, window=60):\n",
    "    \"\"\"Compute rolling Pearson r_t over last window points.\n",
    "\n",
    "    Parameters:\n",
    "    - df: columns ['X', 'Y'] of observations\n",
    "    - window: length of last observations\n",
    "    \n",
    "    Returns:\n",
    "    - r_t: correlation between X and Y over window\n",
    "    \"\"\"\n",
    "\n",
    "    return df['X'].rolling(window).corr(df['Y']).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06c4cc",
   "metadata": {},
   "source": [
    "We can check the empirical mean and covariance before and after tau. The example below shows that the empirical values arein line with the \"expected\" change based on our pre-determined delta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the empirical mean and covariance before and after tau\n",
    "\n",
    "pre = df.iloc[:tau]\n",
    "post = df.iloc[tau:]\n",
    "mean_pre = pre.mean()\n",
    "mean_post = post.mean()\n",
    "cov_pre = pre.cov()\n",
    "cov_post = post.cov()\n",
    "\n",
    "print(\"Pre-change mean:\")\n",
    "print(mean_pre)\n",
    "print(\"\\nPre-change covariance:\")\n",
    "print(cov_pre)\n",
    "\n",
    "print(\"\\nPost-change mean:\")\n",
    "print(mean_post)\n",
    "print(\"\\nPost-change covariance:\")\n",
    "print(cov_post)\n",
    "\n",
    "corr_pre  = cov_pre.loc['X','Y'] / np.sqrt(cov_pre.loc['X','X'] * cov_pre.loc['Y','Y'])\n",
    "corr_post = cov_post.loc['X','Y'] / np.sqrt(cov_post.loc['X','X'] * cov_post.loc['Y','Y'])\n",
    "\n",
    "print(\"\\nEmpirical ρ_pre =\", corr_pre)\n",
    "print(\"Empirical ρ_post =\", corr_post)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a4ee7",
   "metadata": {},
   "source": [
    "## Rolling Correlation Window Size \n",
    "\n",
    "Different window sizes results in varying levels of stability in the rolling correlation, visually revealing the trade-off between stability of values and ability to detect meaningful changes after a changepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c37c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trade off of r's stability and responsiveness to change.\n",
    "\n",
    "w1 = 10\n",
    "w2 = 60\n",
    "w3 = 120\n",
    "\n",
    "r1 = rolling_corr(df, window=w1)\n",
    "r2 = rolling_corr(df, window=w2)\n",
    "r3 = rolling_corr(df, window=w3)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df.index + 1, r1, color='orange', linewidth=1, label=f'Rolling r_t (window={w1})')\n",
    "plt.plot(df.index + 1, r2, color='green', linewidth=1, label=f'Rolling r_t (window={w2})')\n",
    "plt.plot(df.index + 1, r3, color='blue', linewidth=1, label=f'Rolling r_t (window={w3})')\n",
    "plt.axvline(tau, linestyle='--', color='red', label='Changepoint')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rolling Correlation')\n",
    "plt.title('Rolling Correlation over Time')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30409b2",
   "metadata": {},
   "source": [
    "We can quantify this variance in $r_t$ using the closed form variance approximation of $r_t$, which depends on the size of correlation $\\rho$ and the window size $w$.\n",
    "\n",
    "We set $\\rho_0=0.2$ so that a large shift of about $0.7$ from baseline is feasible within $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closed-form variance approximation of r_t over different window sizes, for rho0=0.2.\n",
    "\n",
    "def closed_form_var_rho(w, rho):\n",
    "    return ((1 - rho**2)**2) / (w - 1)\n",
    "\n",
    "rho = 0.2 # same as sample df above.\n",
    "window_sizes = np.array([10, 30, 60, 90, 120])\n",
    "closed_form_vars = closed_form_var_rho(window_sizes, rho)\n",
    "df_closed_form = pd.DataFrame({\n",
    "    \"rho\": rho,\n",
    "    \"Window Size\": window_sizes,\n",
    "    \"Closed-Form Var(r)\": closed_form_vars\n",
    "})\n",
    "\n",
    "df_closed_form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to demonstrate values of a rho closer to 1.\n",
    "\n",
    "rho = 0.9\n",
    "window_sizes = np.array([10, 30, 60, 90, 120])\n",
    "closed_form_vars = closed_form_var_rho(window_sizes, rho)\n",
    "df_closed_form = pd.DataFrame({\n",
    "    \"rho\": rho,\n",
    "    \"Window Size\": window_sizes,\n",
    "    \"Closed-Form Var(r)\": closed_form_vars\n",
    "})\n",
    "\n",
    "df_closed_form\n",
    "\n",
    "# Expected variance of r is closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa4d95",
   "metadata": {},
   "source": [
    "Based on our results, we choose $w=60$, as we are okay with the expected variance of $r_t = 0.0156$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b46082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use window = 60 based on the trade off analysis.\n",
    "\n",
    "r60 = rolling_corr(df, window=60)\n",
    "tau_map = tau\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(r60.index + 1, r60, color='green', linewidth=1, label='Rolling r_t (window=60)')\n",
    "plt.axvline(tau_map, linestyle='--', color='red', label='Changepoint')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rolling Correlation')\n",
    "plt.title('Rolling Correlation over Time')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9e6b9",
   "metadata": {},
   "source": [
    "## Properties of Rolling Correlation\n",
    "\n",
    "We can visualize the distribution of $r_t$ with histograms and q-q plots, which reveal that $r_t$ has a skewed distribution, not normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and qq plot of r with no change shows r is not normally distributed.\n",
    "\n",
    "df, tau = generate_bivariate_stream(mode='control', rho0=0.2, seed=227)\n",
    "window = 60\n",
    "r60 = rolling_corr(df, window=window)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(r60[window:], bins=50, alpha=0.5, label='r with no change')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 6))\n",
    "stats.probplot(r60[window:], dist='norm', plot=ax)\n",
    "ax.set_title('Q–Q Plot: r')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, r with a changepoint is not normally distributed before and after the changepoint.\n",
    "\n",
    "df, tau = generate_bivariate_stream(mode='test', rho0=0.2, seed=227)\n",
    "window = 60\n",
    "r60 = rolling_corr(df, window=window)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(r60[window:tau], bins=50, alpha=0.5, label='r before change')\n",
    "plt.hist(r60[tau:], bins=50, alpha=0.5, label='r after change')\n",
    "plt.title('Overlaid Histograms of $r$ before and after change')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# r is right skewed roughly around rho0 pre change, and left skewed roughly around rho1 post change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff168bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qq plots confirm r is not normal pre and post change.\n",
    "# Adaptive methods may work well for this data given its adaptability.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "stats.probplot(r60[window:tau], dist='norm', plot=axes[0])\n",
    "axes[0].set_title('Q–Q Plot: r (Before Change)')\n",
    "stats.probplot(r60[tau:], dist='norm', plot=axes[1])\n",
    "axes[1].set_title('Q–Q Plot: r (After Change)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25613b1c",
   "metadata": {},
   "source": [
    "We are now ready to dive into the benchmark and proposed methods we will apply to the rolling correlation streams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319252cc",
   "metadata": {},
   "source": [
    "# Benchmark Detectors\n",
    "\n",
    "We discuss the three benchmark detectors evaluated on simulated rolling correlation streams. Details of hyperparameter tuning are provided in the Appendix of the thesis, with key decisions also annotated inline within the code for clarity.\n",
    "\n",
    "## Benchmark Detector 1: CUSUM\n",
    "\n",
    "The CUSUM chart, first proposed by Page (1954), tracks the cumulative sum of deviations from a reference mean and signals an alarm as soon as that sum exceeds a preset threshold. In our setup, the grace period is used to estimate the in‐control mean $\\mu_0$.  \n",
    "\n",
    "The CUSUM recursions are:\n",
    "\n",
    "$S_0^+ = 0, \\quad S_t^+ = \\max\\bigl(0,\\,S_{t-1}^+ + X_t - \\mu_0 - k\\bigr),$  \n",
    "$S_0^- = 0, \\quad S_t^- = \\max\\bigl(0,\\,S_{t-1}^- + \\mu_0 - X_t - k\\bigr),$  \n",
    "\n",
    "and an alarm is raised as soon as $S_t^+ > h$ or $S_t^- > h$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cusum(x, mu0, k, h):\n",
    "    \"\"\"\n",
    "    Run a two-sided CUSUM chart on data x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: 1D array of observations\n",
    "    - mu0: reference (in-control) mean\n",
    "    - k: reference value (typically expected change size divided by 2)\n",
    "    - h: threshold for signaling\n",
    "    \n",
    "    Returns:\n",
    "    - t_alarm: time index when either S_pos > h or S_neg > h, or None if no alarm\n",
    "    \"\"\"\n",
    "    S_pos = 0.0  # Detect positive shifts\n",
    "    S_neg = 0.0  # Detect negative shifts\n",
    "    for t, xt in enumerate(x, start=1):\n",
    "        S_pos = max(0.0, S_pos + (xt - mu0) - k)\n",
    "        S_neg = max(0.0, S_neg + (mu0 - xt) - k)\n",
    "        if S_pos > h or S_neg > h:\n",
    "            return t\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa77c80",
   "metadata": {},
   "source": [
    "We simulate CUSUM detector performance and calibrate parameters based on detection timing and ARL1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip window + burn_in period for calibration & evaluation.\n",
    "\n",
    "def evaluate_cusum_corr(window, h, k, burn_in, n_runs):\n",
    "    # Evaluate ARL0 at h.\n",
    "    fa_times = []\n",
    "    for run in range(n_runs):\n",
    "        df, _ = generate_bivariate_stream(mode='control', seed=run)\n",
    "        S = rolling_corr(df, window=window)\n",
    "        mu0_est = S[window:window+burn_in].mean()\n",
    "        t_fa = detect_cusum(S[window+burn_in:], mu0_est, k, h) or len(S[window+burn_in:]) + 1\n",
    "        fa_times.append(t_fa)\n",
    "    arl0_empirical = np.mean(fa_times)\n",
    "\n",
    "    # Evaluate ARL1 at h, and calculate the detection timing counts.\n",
    "    count_before = count_after = count_never = 0\n",
    "    delays = []\n",
    "    for run in range(n_runs):\n",
    "        df, tau = generate_bivariate_stream(mode='test', seed=run)\n",
    "        S = rolling_corr(df, window=window)\n",
    "        mu0_est = S[window:window+burn_in].mean()\n",
    "        t_alarm = detect_cusum(S[window+burn_in:], mu0_est, k, h)\n",
    "        if t_alarm is None:\n",
    "            count_never += 1\n",
    "        elif t_alarm + window + burn_in < tau: # line up alarm index with original index.\n",
    "            count_before += 1\n",
    "        else:\n",
    "            count_after += 1\n",
    "            delays.append(t_alarm + window + burn_in - tau)\n",
    "\n",
    "    arl1 = np.mean(delays) if delays else float('nan')\n",
    "\n",
    "    return h, arl0_empirical, arl1, count_before, count_after, count_never\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ac792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cusum_corr_params(\n",
    "    burn_in_list, k_list, window=60, h_list=None, n_runs=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate CUSUM on the rolling correlation stream over multiple parameter combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - burn_in_list: list of initial points used to estimate mu0\n",
    "    - k_list: list of expected correlation shifts divided by 2\n",
    "    - window: rolling correlation window size\n",
    "    - h_list: list of CUSUM thresholds\n",
    "    - n_runs: number of Monte Carlo runs\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame indexed by (burn_in, delta, window, h) with empirical ARL0, ARL1,\n",
    "      and counts of alarms: before, after, never\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for burn_in, k, h in itertools.product(burn_in_list, k_list, h_list):\n",
    "        k = k\n",
    "        h_used, arl0_emp, arl1_emp, count_before, count_after, count_never = evaluate_cusum_corr(\n",
    "            window=window,\n",
    "            h=h,\n",
    "            k=k,\n",
    "            burn_in=burn_in,\n",
    "            n_runs=n_runs\n",
    "        )\n",
    "        results.append({\n",
    "            \"burn_in\":        burn_in,\n",
    "            \"window\":         window,\n",
    "            \"h\":              h_used,\n",
    "            \"n_runs\":         n_runs,\n",
    "            \"k\":              k,\n",
    "            \"ARL0_empirical\": arl0_emp,\n",
    "            \"ARL1_empirical\": arl1_emp,\n",
    "            \"count_before\":   count_before,\n",
    "            \"count_after\":    count_after,\n",
    "            \"count_never\":    count_never\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index(['burn_in', 'k', 'window', 'h'], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary burn-in and h. May improve mu0 estimate and reduce early alerts.\n",
    "# Since mu0 is estimated between window and window+burn_in, the length of mu0 estimation is simply burn_in.\n",
    "\n",
    "results_df = evaluate_cusum_corr_params(\n",
    "    burn_in_list=[30, 60, 90],\n",
    "    k_list=[0.35], # Try expected change / 2\n",
    "    window=60,\n",
    "    h_list=[0.5, 0.6, 0.7, 0.8],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Longer burn in period for estimating mu0 of r seems to work best, reducing early alerts.\n",
    "# Also, there are ~30 days delay (ARL1) which is reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4df30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary k.\n",
    "\n",
    "results_df = evaluate_cusum_corr_params(\n",
    "    burn_in_list=[90],\n",
    "    k_list=[0.25, 0.35, 0.45],\n",
    "    window=60,\n",
    "    h_list=[0.6, 0.7, 0.8],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Larger delta seems to work best, regardless of h ranging between 0.6 and 0.8.\n",
    "# So in real life, we may want to test a few different CUSUM variable k based on known historical events in order to set k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params: burn in 90, k = 0.45, h = 0.7\n",
    "\n",
    "results_df = evaluate_cusum_corr_params(\n",
    "    burn_in_list=[90],\n",
    "    k_list=[0.45],\n",
    "    window=60,\n",
    "    h_list=[0.7],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec896fa",
   "metadata": {},
   "source": [
    "## Benchmark Detector 2: Wilcoxon Ranksum Sequential Test\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ranksums.html\n",
    "\n",
    "The Wilcoxon rank-sum test (Wilcoxon, 1945) is a nonparametric method for detecting differences between two independent samples. \n",
    "\n",
    "In our scan-based framework, the rolling correlation series is retrospectively split at every possible point within a historical window, subject to minimum window size. For each candidate split, the test statistic comparing the ranks before and after the split is computed. We then take the maximum absolute test statistic across all splits and compare it to a fixed threshold. \n",
    "\n",
    "An alarm is triggered when this maximum exceeds the threshold $H_W$, indicating a likely changepoint. This method adapts to shifts of unknown timing and is robust to non-normality, skewness, and heavy tails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RankSum and also threshold value instead of pvalue which is more sensitive to correlation.\n",
    "\n",
    "def detect_ranksum(x, w_min=20, H=10.0):\n",
    "    \"\"\"\n",
    "    Run a Wilcoxon Ranksum sliding-window detector on data x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: 1D array of observations\n",
    "    - w_min: minimum comparison window size\n",
    "    - H: threshold for signaling\n",
    "    \n",
    "    Returns:\n",
    "    - t_alarm: time index when zmax >= H, or None if no alarm\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    n = len(x)\n",
    "    stat = np.full(n, np.nan)\n",
    "    for t in range(2*w_min, n+1):\n",
    "        z = [abs(ranksums(x[:k], x[k:t]).statistic) for k in range(w_min, t-w_min+1)]\n",
    "        zmax = np.max(z)\n",
    "        stat[t-1] = zmax\n",
    "        if zmax >= H:\n",
    "            return t\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b2b70",
   "metadata": {},
   "source": [
    "We simulate WRS detector performance and calibrate parameters.\n",
    "\n",
    "First, we try the detector on a single $r_t$ stream, where $\\tau=241$ (true changepoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc678824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, tau2 = generate_bivariate_stream(rho0=0.2, seed=150)\n",
    "r60_v2 = rolling_corr(df2, window=60)\n",
    "tau_map_v2 = tau2\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(r60_v2.index + 1, r60_v2, color='green', linewidth=1, label='Rolling r_t (window=60)')\n",
    "plt.axvline(tau_map_v2, linestyle='--', color='red', label='Changepoint')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rolling Correlation')\n",
    "plt.title('Rolling Correlation over Time')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(tau2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423534be",
   "metadata": {},
   "source": [
    "We try a few different ranges of $H$, landing on $H=10.2$ which results in a detection at $t=243$, only 2 days after the true $\\tau$. \n",
    "\n",
    "Minimum window size $10$ and $30$ results in the same first detection. To reduce computational costs, we choose minimum window $30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 60\n",
    "r60 = rolling_corr(df2, window=window)\n",
    "w_min = 10\n",
    "H = 10.2\n",
    "\n",
    "detect_ranksum(r60[window:], w_min=w_min, H=H) + window # Realign index to r60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 60\n",
    "r60 = rolling_corr(df2, window=window)\n",
    "w_min = 30\n",
    "H = 10.2\n",
    "\n",
    "detect_ranksum(r60[window:], w_min=w_min, H=H) + window # Realign index to r60.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f9b71",
   "metadata": {},
   "source": [
    "We calibrate at scale now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranksum_corr(window, w_min, H, n_runs):\n",
    "    # Evaluate ARL0 at H\n",
    "    fa_times = []\n",
    "    for run in range(n_runs):\n",
    "        df, _ = generate_bivariate_stream(mode='control', seed=run)\n",
    "        S = rolling_corr(df, window=window)\n",
    "        t_fa = detect_ranksum(S[window:], w_min=w_min, H=H)\n",
    "        t_fa_scalar = t_fa if t_fa is not None else len(S[window:]) + 1\n",
    "        fa_times.append(t_fa_scalar)\n",
    "    arl0_empirical = np.mean(fa_times)\n",
    "\n",
    "    count_before = count_after = count_never = 0\n",
    "    delays = []\n",
    "    for run in range(n_runs):\n",
    "        df, tau = generate_bivariate_stream(mode='test', seed=run)\n",
    "        S = rolling_corr(df, window=window)\n",
    "        t_alarm = detect_ranksum(S[window:], w_min=w_min, H=H)\n",
    "        if t_alarm is None:\n",
    "            count_never += 1\n",
    "        elif t_alarm + window < tau:\n",
    "            count_before += 1\n",
    "        else:\n",
    "            count_after += 1\n",
    "            delays.append(t_alarm + window - tau)\n",
    "    arl1_empirical = np.mean(delays) if delays else float('nan')\n",
    "\n",
    "    return H, arl0_empirical, arl1_empirical, count_before, count_after, count_never\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranksum_corr_params(w_min_list, H_list, window=60, n_runs=10):\n",
    "    \"\"\"\n",
    "    Evaluate Wilcoxon Ranksum sliding-window detector on the rolling correlation stream\n",
    "    over multiple parameter combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - w_min_list: list of minimum comparison window sizes.\n",
    "    - H_list: list of test thresholds.\n",
    "    - window: rolling correlation window size\n",
    "    - n_runs: number of Monte Carlo runs.\n",
    "\n",
    "    Returns:\n",
    "    - Data frame indexed by (w_min, H) with empirical ARL0, ARL1,\n",
    "      and counts of alarms: before, after, never.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for w_min, H in itertools.product(w_min_list, H_list):\n",
    "        H_used, arl0_emp, arl1_emp, count_before, count_after, count_never = evaluate_ranksum_corr(\n",
    "            window=window,\n",
    "            w_min=w_min,\n",
    "            H=H,\n",
    "            n_runs=n_runs\n",
    "        )\n",
    "        results.append({\n",
    "            \"window\":         window,\n",
    "            \"w_min\":          w_min,\n",
    "            \"H\":              H_used,\n",
    "            \"n_runs\":         n_runs,\n",
    "            \"ARL0_empirical\": arl0_emp,\n",
    "            \"ARL1_empirical\": arl1_emp,\n",
    "            \"count_before\":   count_before,\n",
    "            \"count_after\":    count_after,\n",
    "            \"count_never\":    count_never\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index(['w_min', 'H'], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary w_min size\n",
    "\n",
    "results_df = evaluate_ranksum_corr_params(\n",
    "    w_min_list = [10, 30, 60, 100],\n",
    "    H_list = [10.2], # Same H value as before when we applied the detector on a single stream.\n",
    "    window = 60,\n",
    "    n_runs = 10\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Took 2.5 min.\n",
    "# Early detection is common for all w_min other than 100, so try more values closer to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further vary w_min size.\n",
    "\n",
    "results_df = evaluate_ranksum_corr_params(\n",
    "    w_min_list = [80, 85, 90],\n",
    "    H_list = [10.2], # Same H value as before when we applied the detector on a single stream.\n",
    "    window = 60,\n",
    "    n_runs = 10\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Took 1 min.\n",
    "# w_min = 85 has no early detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary H to calibrate to a comparable ARL0 as other methods.\n",
    "\n",
    "results_df = evaluate_ranksum_corr_params(\n",
    "    w_min_list = [85],\n",
    "    H_list = [5, 10, 15],\n",
    "    window = 60,\n",
    "    n_runs = 10\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Took 4 min.\n",
    "# H=5 is too low and clearly results in false early detections.\n",
    "# H=10 is still a bit low where 2 out of 5 runs result in early detections.\n",
    "# H=15 is too high and ARL1 is 139 days, though all runs detect after true tau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary H.\n",
    "\n",
    "results_df = evaluate_ranksum_corr_params(\n",
    "    w_min_list = [85],\n",
    "    H_list = [10, 11, 12, 13, 14, 15],\n",
    "    window = 60,\n",
    "    n_runs = 10\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Took 5.5 min\n",
    "# Results show that between H=12 and 13 we should be able to find a comparable ARL0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hone in on H values between 12 and 13.\n",
    "\n",
    "results_df = evaluate_ranksum_corr_params(\n",
    "    w_min_list = [85],\n",
    "    H_list = [12.3, 12.4, 12.5],\n",
    "    window = 60,\n",
    "    n_runs = 10\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Took 2.3 min\n",
    "# H = 12.4 seems to align best with other method ARL0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54645d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon ranksum best params, for n_runs = 10.\n",
    "\n",
    "results_df = evaluate_ranksum_corr_params(\n",
    "    w_min_list = [85],\n",
    "    H_list = [12.4],\n",
    "    window = 60,\n",
    "    n_runs = 10\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Took 1.5 min."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966fc8c",
   "metadata": {},
   "source": [
    "## Benchmark Detector 3: Multivariate EWMA\n",
    "\n",
    "The Multivariate EWMA (MEWMA) chart (Lowry, 1992) is an extension of EWMA to multivariate data, allowing joint monitoring of multiple series and their covariance structure. Rather than operating on the derived correlation series $\\{r_t\\}$, MEWMA monitors the raw bivariate pair $(X_t, Y_t)$ directly. Each series is standardized using the mean and variance estimated from the burn-in period, ensuring that departures flagged by MEWMA primarily reflect changes in covariance rather than shifts in marginal scale. The monitoring statistic is Hotelling’s $T^2$,\n",
    "$$W_t = \\mathbf{T}_t^\\top \\Sigma_T^{-1}\\mathbf{T}_t,$$\n",
    "where $\\Sigma_T$ is the in‐control covariance of $\\mathbf{T}_t$. \n",
    "A control limit $H_M$ is set by Monte Carlo calibration to achieve the desired ARL$_0$, and an alarm is raised whenever $W_t > H_M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mewma(x, y, burn_in=60, lam=0.2, H=10.0):\n",
    "    \"\"\"\n",
    "    Run a MEWMA chart on data (x, y).\n",
    "\n",
    "    Parameters:\n",
    "    - x: 1D array of observations for the first series\n",
    "    - y: 1D array of observations for the second series (same length as x)\n",
    "    - burn_in: number of initial observations used to estimate in-control mean/variance and covariance\n",
    "    - lam: MEWMA smoothing parameter in (0, 1]\n",
    "    - H: threshold for signaling\n",
    "\n",
    "    Returns:\n",
    "    - t_alarm: time index when W_t > H, or None if no alarm\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(x)\n",
    "    eps = 1e-12\n",
    "\n",
    "    # Burn-in mean & std\n",
    "    mx, sx = np.mean(x[:burn_in]), np.std(x[:burn_in], ddof=1)\n",
    "    my, sy = np.mean(y[:burn_in]), np.std(y[:burn_in], ddof=1)\n",
    "    sx = max(sx, eps); sy = max(sy, eps)\n",
    "    Xs = (x - mx) / sx\n",
    "    Ys = (y - my) / sy\n",
    "    Z = np.column_stack([Xs, Ys])\n",
    "\n",
    "    # In-control covariance and MEWMA state covariance\n",
    "    Sigma0 = np.cov(Z[:burn_in].T, ddof=1)\n",
    "    SigmaT = (lam / (2.0 - lam)) * Sigma0\n",
    "    try:\n",
    "        SigmaT_inv = np.linalg.inv(SigmaT)\n",
    "    except np.linalg.LinAlgError:\n",
    "        SigmaT_inv = np.linalg.pinv(SigmaT + 1e-10 * np.eye(2))\n",
    "\n",
    "    T = np.zeros(2)\n",
    "    for t in range(burn_in, n):\n",
    "        T = lam * Z[t] + (1.0 - lam) * T\n",
    "        W_t = float(T @ SigmaT_inv @ T)\n",
    "        if W_t > H:\n",
    "            return t\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60c3f1",
   "metadata": {},
   "source": [
    "We simulate MEWMA detector performance and calibrate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65847f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mewma(lam=0.5, burn_in=60, n_runs=200, H=None):\n",
    "    \"\"\"\n",
    "    Evaluate MEWMA ARL0, ARL1, and count alarms before/after/never relative to the true tau.\n",
    "    \"\"\"\n",
    "    # Empirical ARL0 at H\n",
    "    fa_times = []\n",
    "    for run in range(n_runs):\n",
    "        df, _ = generate_bivariate_stream(mode='control', seed=run)\n",
    "        x, y = df.iloc[:, 0].values, df.iloc[:, 1].values\n",
    "        t_fa = detect_mewma(x, y, burn_in=burn_in, lam=lam, H=H) or len(x) + 1\n",
    "        fa_times.append(t_fa)\n",
    "    arl0_empirical = np.mean(fa_times)\n",
    "\n",
    "    # Empirical ARL1 at H, plus before/after/never counts\n",
    "    count_before = count_after = count_never = 0\n",
    "    delays = []\n",
    "    for run in range(n_runs):\n",
    "        df, tau = generate_bivariate_stream(mode='test', seed=run)\n",
    "        x, y = df.iloc[:, 0].values, df.iloc[:, 1].values\n",
    "        t_alarm = detect_mewma(x, y, burn_in=burn_in, lam=lam, H=H)\n",
    "        if t_alarm is None:\n",
    "            count_never += 1\n",
    "        elif t_alarm < tau:\n",
    "            count_before += 1\n",
    "        else:\n",
    "            count_after += 1\n",
    "            delays.append(t_alarm - tau)\n",
    "\n",
    "    arl1_empirical = np.mean(delays) if delays else float('nan')\n",
    "\n",
    "    return H, lam, arl0_empirical, arl1_empirical, count_never, count_before, count_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mewma_params(burn_in_list, lam_list, H_list, n_runs=200):\n",
    "    \"\"\"\n",
    "    Evaluate MEWMA on the bivariate streams over multiple parameter combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - burn_in_list: list of initial points used to estimate in-control mean/var and covariance\n",
    "    - lam_list: list of MEWMA smoothing parameters\n",
    "    - H_list: list of MEWMA thresholds.\n",
    "    - n_runs: number of Monte Carlo runs.\n",
    "\n",
    "    Returns:\n",
    "    - Dataframe indexed by (burn_in, lam, H) with empirical ARL0, ARL1,\n",
    "      and counts of alarms: before, after, never\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for burn_in, lam, H in itertools.product(burn_in_list, lam_list, H_list):\n",
    "        H_used, lam_used, arl0_emp, arl1_emp, count_never, count_before, count_after = evaluate_mewma(\n",
    "            lam=lam, burn_in=burn_in, n_runs=n_runs, H=H\n",
    "        )\n",
    "        results.append({\n",
    "            \"burn_in\":        burn_in,\n",
    "            \"lam\":            lam_used,\n",
    "            \"H\":              H_used,\n",
    "            \"n_runs\":         n_runs,\n",
    "            \"ARL0_empirical\": arl0_emp,\n",
    "            \"ARL1_empirical\": arl1_emp,\n",
    "            \"count_before\":   count_before,\n",
    "            \"count_after\":    count_after,\n",
    "            \"count_never\":    count_never,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index([\"burn_in\", \"lam\", \"H\"], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary burn in.\n",
    "\n",
    "burn_in_list = [30, 60, 90]\n",
    "lam_list = [0.2]\n",
    "H_list = [10, 15]\n",
    "n_runs = 200\n",
    "\n",
    "df_results = evaluate_mewma_params(burn_in_list=burn_in_list, lam_list=lam_list, H_list=H_list, n_runs=n_runs)\n",
    "\n",
    "df_results\n",
    "\n",
    "# Nothing is too great but burn in = 90 seems to get shorter ARL1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary H.\n",
    " \n",
    "burn_in_list = [90]\n",
    "lam_list = [0.2]\n",
    "H_list = [10, 11, 12, 13, 14, 15]\n",
    "n_runs = 200\n",
    "\n",
    "df_results = evaluate_mewma_params(burn_in_list=burn_in_list, lam_list=lam_list, H_list=H_list, n_runs=n_runs)\n",
    "\n",
    "df_results\n",
    "\n",
    "# 13 results in 39 early, 121 after, and 40 never. this is still not great. but best out of the 6 H values if we add early and never and compare across H values, H=13 has the smallest value.\n",
    "#   H=10: 112 + 2 = 114,\n",
    "#   H=11: 81 + 10 = 91,\n",
    "#   H=12: 57 + 117 = 174,\n",
    "#   H=13: 39 + 40 = 49,\n",
    "#   H=14: 28 + 52 = 80,\n",
    "#   H=15: 20 + 110 = 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07486a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary lambda.\n",
    " \n",
    "burn_in_list = [90]\n",
    "lam_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "H_list = [13]\n",
    "n_runs = 200\n",
    "\n",
    "df_results = evaluate_mewma_params(burn_in_list=burn_in_list, lam_list=lam_list, H_list=H_list, n_runs=n_runs)\n",
    "\n",
    "df_results\n",
    "\n",
    "# lambda = 0.4 seems best based on same criteria of early + never."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary burn in again.\n",
    "\n",
    "burn_in_list = [90, 100, 110, 120]\n",
    "lam_list = [0.4]\n",
    "H_list = [13]\n",
    "n_runs = 200\n",
    "\n",
    "df_results = evaluate_mewma_params(burn_in_list=burn_in_list, lam_list=lam_list, H_list=H_list, n_runs=n_runs)\n",
    "\n",
    "df_results\n",
    "\n",
    "# burn in = 110 is best based on the same early + never comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbac142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params.\n",
    "\n",
    "burn_in_list = [110]\n",
    "lam_list = [0.4]\n",
    "H_list = [13]\n",
    "n_runs = 200\n",
    "\n",
    "df_results = evaluate_mewma_params(burn_in_list=burn_in_list, lam_list=lam_list, H_list=H_list, n_runs=n_runs)\n",
    "\n",
    "df_results\n",
    "\n",
    "# even \"best\" params do result in 15.5% early detection out of 200 Monte Carlo simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55234f",
   "metadata": {},
   "source": [
    "# Proposed Method: Adaptive EWMA\n",
    "\n",
    "We now implement the proposed method, adaptive EWMA, which extends the classical exponentially weighted moving average by updating its forgetting factor $\\lambda_t$ online in response to recent forecast errors, allowing the chart to adapt its memory to evolving data. Let $x_t$ be the monitored series (e.g., rolling correlation $r_t$) and $\\hat{\\mu}_t$ its smoothed estimate. Starting from a burn‐in estimate $\\mu_0$, the one‐step prediction error is  \n",
    "\n",
    "$$ e_t = x_t - \\hat{\\mu}_{t-1}.$$\n",
    "\n",
    "An auxiliary accumulator $g_t$ tracks the sensitivity of the smoothed mean to $\\lambda_t$ via\n",
    "\n",
    "$$ g_t = -e_t + \\lambda_{t-1} g_{t-1}. $$\n",
    "\n",
    "The forgetting factor is then updated with a clipped gradient step,  \n",
    "\n",
    "$$ \\lambda_t = \\min\\{1, \\max \\{0, \\lambda_{t-1} + \\eta e_t g_t\\}\\}, $$\n",
    "\n",
    "where $\\eta>0$ controls the adaptation speed. Larger $\\eta$ yields faster response to change, while smaller values provide stability in steady regimes. The smoothed mean is updated as in a standard EWMA,  \n",
    "\n",
    "$$\\hat{\\mu}_t = \\lambda_t \\hat{\\mu}_{t-1} + (1 - \\lambda_t) x_t.$$\n",
    "\n",
    "An alarm is raised when the deviation from the in‐control mean exceeds a symmetric control limit,  \n",
    "\n",
    "$$|\\hat{\\mu}_t - \\mu_0| > h.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_adaptive_ewma(x, mu0, eta=0.01, h=1.0, lam=0.5):\n",
    "    \"\"\"\n",
    "    Run a two-sided Adaptive EWMA detector on data x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: 1D array of observations\n",
    "    - mu0: reference (in-control) mean\n",
    "    - eta: learning rate for adapting the forgetting factor\n",
    "    - h: symmetric control limit around mu0\n",
    "    - lam: Adaptive EWMA smoothing parameter in (0, 1]\n",
    "    \n",
    "    Returns:\n",
    "    - t_alarm: time index (1-based) when |mu_hat - mu0| > h, or None if no alarm\n",
    "    \"\"\"\n",
    "    mu_hat = mu0\n",
    "    lam = lam\n",
    "    g = 0.0\n",
    "    \n",
    "    for t, xt in enumerate(x, start=1):\n",
    "        e = xt - mu_hat\n",
    "        g = -e + lam * g\n",
    "        lam = np.clip(lam + eta * e * g, 0.0, 1.0)\n",
    "        mu_hat = lam * mu_hat + (1 - lam) * xt\n",
    "        if abs(mu_hat - mu0) > h:\n",
    "            return t\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3bfd9",
   "metadata": {},
   "source": [
    "We simulate A-EWMA detector performance and calibrate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6bc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_ewma_corr(window, eta=0.1, burn_in=50, n_runs=200, h=None, lam=0.5):\n",
    "    # Empirical ARL0 at h\n",
    "    fa_times = []\n",
    "    for run in range(n_runs):\n",
    "        df, _ = generate_bivariate_stream(mode='control', seed=run)\n",
    "        S = rolling_corr(df, window)\n",
    "        mu0_est = S[window:window+burn_in].mean()\n",
    "        t_fa = detect_adaptive_ewma(S[window+burn_in:].values, mu0_est, eta, h, lam) or len(S[window+burn_in:]) + 1\n",
    "        fa_times.append(t_fa)\n",
    "    arl0_empirical = np.mean(fa_times)\n",
    "\n",
    "    # Empirical ARL1 at h, plus before/after/never counts\n",
    "    count_before = count_after = count_never = 0\n",
    "    delays = []\n",
    "    for run in range(n_runs):\n",
    "        df, tau = generate_bivariate_stream(mode='test', seed=run)\n",
    "        S = rolling_corr(df, window)\n",
    "        mu0_est = S[window:window+burn_in].mean()\n",
    "        t_alarm = detect_adaptive_ewma(S[window+burn_in:], mu0_est, eta, h, lam)\n",
    "        if t_alarm is None:\n",
    "            count_never += 1\n",
    "        elif t_alarm + window + burn_in < tau:\n",
    "            count_before += 1\n",
    "        else:\n",
    "            count_after += 1\n",
    "            delays.append(t_alarm + window + burn_in - tau)\n",
    "\n",
    "    arl1_empirical = np.mean(delays) if delays else float('nan')\n",
    "\n",
    "    return h, eta, lam, arl0_empirical, arl1_empirical, count_never, count_before, count_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b147c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_ewma_corr_params(burn_in_list, h_list, eta_list, lam_list, window=60, n_runs=200):\n",
    "    \"\"\"\n",
    "    Evaluate A-EWMA CUSUM on rolling correlation stream over multiple parameter combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - burn_in_list: list of initial points used to estimate mu0\n",
    "    - h_list: list of A-EWMA thresholds\n",
    "    - eta_list: list of A-EWMA gradient learning rate\n",
    "    - lam_list: list of A-EWMA smoothing parameter\n",
    "    - window: rolling correlation window size\n",
    "    - n_runs: number of Monte Carlo runs\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame indexed by (burn_in, h, eta, lam) with empirical ARL0, ARL1,\n",
    "      and counts of alarms: before, after, never\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for burn_in, h, eta, lam in itertools.product(burn_in_list, h_list, eta_list, lam_list):\n",
    "        h_used, eta, lam, arl0_emp, arl1_emp, count_never, count_before, count_after = evaluate_adaptive_ewma_corr(\n",
    "            window=window,\n",
    "            eta=eta,\n",
    "            burn_in=burn_in,\n",
    "            n_runs=n_runs,\n",
    "            h=h,\n",
    "            lam=lam\n",
    "        )\n",
    "        results.append({\n",
    "            \"window\":         window,\n",
    "            \"burn_in\":        burn_in,\n",
    "            \"h\":              h_used,\n",
    "            \"eta\":            eta,\n",
    "            \"lam\":            lam,\n",
    "            \"n_runs\":         n_runs,\n",
    "            \"ARL0_empirical\": arl0_emp,\n",
    "            \"ARL1_empirical\": arl1_emp,\n",
    "            \"count_before\":   count_before,\n",
    "            \"count_after\":    count_after,\n",
    "            \"count_never\":    count_never\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index(['burn_in', 'window', 'h'], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a57197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try same range of burn in as CUSUM, with some h values.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[30, 60, 90],\n",
    "    window=60,\n",
    "    h_list=[0.3, 0.5, 0.7],\n",
    "    eta_list=[0.01],\n",
    "    lam_list=[0.5],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Burn in 30 seems to already achieve low early alarms.\n",
    "# But ARL0 (424) is a bit higher than CUSUM (379) / WRS (322)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a few different h between 0.3 and 0.5 to find comparable ARL0.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[30],\n",
    "    window=60,\n",
    "    h_list=[0.4, 0.45, 0.5],\n",
    "    eta_list=[0.01],\n",
    "    lam_list=[0.5],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Lowering ARL0 via lower h increases early detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c42e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try burn in 60, with a few different h between 0.3 and 0.5 to find comparable ARL0.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[60],\n",
    "    window=60,\n",
    "    h_list=[0.4, 0.45, 0.5],\n",
    "    eta_list=[0.01],\n",
    "    lam_list=[0.5],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# We ideally want ARL0 closer to the other detectors (380s), without increasing early detection to more than 5. So burn in 60 is also not the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14716a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try burn in 90, with a few different h between 0.3 and 0.5 to find comparable ARL0.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[90],\n",
    "    window=60,\n",
    "    h_list=[0.4, 0.45, 0.5],\n",
    "    eta_list=[0.01],\n",
    "    lam_list=[0.5],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# burn in 90 and h=0.5 produces a comparable ARL0 (379) with fewer than 5 early detections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary eta.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[90],\n",
    "    window=60,\n",
    "    h_list=[0.5],\n",
    "    eta_list=[0.001, 0.01, 0.1],\n",
    "    lam_list=[0.5],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Eta value doesn't seem to matter too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe05d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different lambdas.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[90],\n",
    "    window=60,\n",
    "    h_list=[0.5],\n",
    "    eta_list=[0.01],\n",
    "    lam_list=[0.1, 0.5, 0.9],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df\n",
    "\n",
    "# Lambda doesn't seem to matter too much. This is good since we may not need to tune in life. Just use 0.5 and assume will work fine.\n",
    "# So the only thing we would need to tune is h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43721ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use burn in 90, h=0.5, eta=0.01, lam=0.5 since these all worked well.\n",
    "\n",
    "results_df = evaluate_adaptive_ewma_corr_params(\n",
    "    burn_in_list=[90],\n",
    "    window=60,\n",
    "    h_list=[0.5],\n",
    "    eta_list=[0.01],\n",
    "    lam_list=[0.5],\n",
    "    n_runs=200\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45338008",
   "metadata": {},
   "source": [
    "# Real-World Case Study\n",
    "\n",
    "Now we apply the detectors to real-world cases, using public datasets from Kagglehub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48135abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new rolling correlation function that allows two separate series not part of the same df.\n",
    "\n",
    "def rolling_corr_test(X, Y, window=60):\n",
    "    return X.rolling(window=window).corr(Y).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd42ad",
   "metadata": {},
   "source": [
    "## Case Study 1: NVDA vs GOOGL\n",
    "\n",
    "We look at closing price of NVDA and GOOGL between July 2019 and July 2021.\n",
    "\n",
    "(Source: https://www.kaggle.com/datasets/evangower/big-tech-stock-prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "\n",
    "dataset = kagglehub.dataset_download(\"evangower/big-tech-stock-prices\")\n",
    "X_df = pd.read_csv(dataset + \"/NVDA.csv\")\n",
    "Y_df = pd.read_csv(dataset + \"/GOOGL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date range.\n",
    "\n",
    "X_df = X_df[(X_df['Date'] >= '2019-07-01') & (X_df['Date'] < '2021-07-01')]\n",
    "Y_df = Y_df[(Y_df['Date'] >= '2019-07-01') & (Y_df['Date'] < '2021-07-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "\n",
    "X_df['Date'] = pd.to_datetime(X_df['Date'])\n",
    "Y_df['Date'] = pd.to_datetime(Y_df['Date'])\n",
    "X_time = X_df['Date']\n",
    "Y_time = Y_df['Date']\n",
    "X = X_df['Close']\n",
    "Y = Y_df['Close']\n",
    "x_label = 'NVDA'\n",
    "y_label = 'GOOGL'\n",
    "metric_name = \"Closing Price (USD)\"\n",
    "title = f'{x_label} vs {y_label} {metric_name} Over Time'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(X_time, X, label=x_label, alpha=0.8)\n",
    "plt.plot(Y_time, Y, label=y_label, alpha=0.8)\n",
    "plt.title(title, fontsize=18)\n",
    "plt.xlabel(\"Date\",fontsize=12)\n",
    "plt.ylabel(metric_name,fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75001b3f",
   "metadata": {},
   "source": [
    "We can also visualize the time series with the rolling correlation overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f182fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rolling correlation along with the two data streams.\n",
    "\n",
    "window = 60\n",
    "r = rolling_corr_test(X, Y, window=window)\n",
    "r_label = 'Rolling Correlation (w=60)'\n",
    "\n",
    "# First y-axis for prices\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.plot(X_time.iloc[window:], X.iloc[window:], label=x_label, alpha=0.8)\n",
    "ax1.plot(Y_time.iloc[window:], Y.iloc[window:], label=y_label, alpha=0.8)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(metric_name)\n",
    "ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Second y-axis for rolling correlation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(X_time.iloc[window:], r.iloc[window:], label=r_label, alpha=0.8, color='purple')\n",
    "ax2.set_ylabel(\"Rolling Correlation (r)\")\n",
    "ax2.set_ylim(-1, 1)  # Force correlation scale\n",
    "\n",
    "fig.suptitle(title)\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='center left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b4a7a",
   "metadata": {},
   "source": [
    "Now, we apply the four detectors using the calibrated parameters from simulations, and map the results onto the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e020695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity define data explicitly again.\n",
    "\n",
    "X = X_df['Close']\n",
    "Y = Y_df['Close']\n",
    "window = 60\n",
    "S = rolling_corr_test(X=X, Y=Y, window=window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSUM\n",
    "\n",
    "burn_in = 90; h = 0.7; k = 0.45\n",
    "\n",
    "mu0_est = S.iloc[window:window+burn_in].mean()\n",
    "data = S.iloc[window+burn_in:]\n",
    "\n",
    "cusum_tau = detect_cusum(data, mu0_est, k, h)\n",
    "cusum_tau += window + burn_in\n",
    "cusum_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb986c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon Ranksum\n",
    "\n",
    "w_min = 85; H = 12.4\n",
    "\n",
    "data = S.iloc[window:]\n",
    "\n",
    "ranksum_tau = detect_ranksum(x=data, w_min=w_min, H=H)[0]\n",
    "ranksum_tau += window\n",
    "ranksum_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEWMA\n",
    "\n",
    "burn_in = 110; lam = 0.4; H = 13\n",
    "\n",
    "mewma_tau = detect_mewma(x=X, y=Y, burn_in=burn_in, lam=lam, H=H)\n",
    "mewma_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive EWMA\n",
    "\n",
    "burn_in = 90; h = 0.5; eta = 0.01; lam = 0.5\n",
    "\n",
    "mu0_est = S.iloc[window:window+burn_in].mean()\n",
    "data = S.iloc[window+burn_in:]\n",
    "\n",
    "adaptive_ewma_tau = detect_adaptive_ewma(data, mu0=mu0_est, eta=eta, h=h, lam=lam)\n",
    "adaptive_ewma_tau += window + burn_in\n",
    "adaptive_ewma_tau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f07bee",
   "metadata": {},
   "source": [
    "Based on the \"best\" parameters from simulated study, it seems that CUSUM, A-EWMA, and WRS are detecting a change at around 0.7 correlation point drop as calibrated during simulations, but with varying degrees of delay. However, MEWMA detected change much earlier than the other three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_idx = X_time.iloc[window:]  # align dates with plotted slices\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "ax1.plot(time_idx, X.iloc[window:], label=x_label, alpha=0.8)\n",
    "ax1.plot(time_idx, Y.iloc[window:], label=y_label, alpha=0.8)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(metric_name)\n",
    "ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(time_idx, r.iloc[window:], label=r_label, alpha=0.8, color='purple')\n",
    "ax2.set_ylabel(\"Rolling Correlation (r)\")\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "# Vertical lines at detected changepoints (map indices to dates)\n",
    "ax1.axvline(X_time.iloc[cusum_tau], color='red', linestyle=':', label='CUSUM τ', linewidth=2, alpha=0.9)\n",
    "ax1.axvline(X_time.iloc[adaptive_ewma_tau], color='blue', linestyle=':', label='Adaptive EWMA τ', linewidth=2, alpha=0.9)\n",
    "ax1.axvline(X_time.iloc[ranksum_tau], color='green', linestyle=':', label=\"Wilcoxon Ranksum τ\", linewidth=2, alpha=0.9)\n",
    "ax1.axvline(X_time.iloc[mewma_tau], color='darkorange', linestyle=':', label='MEWMA τ', linewidth=2, alpha=0.9)\n",
    "\n",
    "fig.suptitle(title, fontsize=18)\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1 + h2, l1 + l2, loc='center left', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43eb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detection times.\n",
    "\n",
    "taus = {\n",
    "    \"CUSUM\": cusum_tau,\n",
    "    \"A-EWMA\": adaptive_ewma_tau,\n",
    "    \"Wilcoxon\": ranksum_tau,\n",
    "    \"MEWMA\": mewma_tau\n",
    "}\n",
    "\n",
    "summary = [\n",
    "    (m, t, X_time.iloc[t].date(), float(r.iloc[t])) for m, t in taus.items()\n",
    "]\n",
    "\n",
    "summary.sort(key=lambda row: row[1])\n",
    "\n",
    "for row in summary:\n",
    "    print(f\"{row[0]:9s} t={row[1]:4d}  date={row[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6b773",
   "metadata": {},
   "source": [
    "## Case Study 2: COVID-19 in LA vs. SF\n",
    "\n",
    "Next, we look at COVID-19 daily cases in LA vs. SF counties between March 2020 and March 2021. \n",
    "\n",
    "(Source: https://www.kaggle.com/datasets/fireballbyedimyrnmom/us-counties-covid-19-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "\n",
    "dataset = kagglehub.dataset_download(\"fireballbyedimyrnmom/us-counties-covid-19-dataset\")\n",
    "df = pd.read_csv(dataset + \"/us-counties.csv\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "counties = ['Los Angeles', 'San Francisco']\n",
    "df = df.loc[\n",
    "            (df['state'] == 'California') & (df['county'].isin(counties)),\n",
    "            ['date', 'county', 'cases']\n",
    "        ].groupby(['date', 'county'], as_index=False)['cases'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily cases per 10k capita and pivot.\n",
    "\n",
    "county_population = {\n",
    "    'Los Angeles': 9996634,\n",
    "    'San Francisco': 874826,\n",
    "}\n",
    "\n",
    "cases_df = df.sort_values(['county', 'date'])\n",
    "cases_df['new_cases'] = cases_df.groupby('county')['cases'].diff().fillna(0)\n",
    "cases_df['new_cases'] = cases_df['new_cases'].clip(lower=0)\n",
    "cases_df['per100k'] = cases_df.apply(lambda r: (r['new_cases'] / county_population[r['county']]) * 100_000, axis=1)\n",
    "cases_df = cases_df.pivot(index='date', columns='county', values='per100k').sort_index()\n",
    "cases_df = cases_df.loc['2020-03-01':'2021-03-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffaa9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rolling correlation along with the two data streams.\n",
    "\n",
    "time_idx = cases_df.index\n",
    "X = cases_df['Los Angeles']\n",
    "Y = cases_df['San Francisco']\n",
    "x_label = 'Los Angeles'\n",
    "y_label = 'San Francisco'\n",
    "metric_name = \"New COVID-19 Cases\"\n",
    "title = f'{x_label} vs {y_label} {metric_name} Over Time'\n",
    "\n",
    "window = 60\n",
    "r = rolling_corr_test(X, Y, window=window)\n",
    "r_label = 'Rolling Correlation (w=60)'\n",
    "\n",
    "\n",
    "# First y-axis for prices\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.plot(time_idx[window:], X.iloc[window:], label=x_label, alpha=0.8)\n",
    "ax1.plot(time_idx[window:], Y.iloc[window:], label=y_label, alpha=0.8)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(metric_name)\n",
    "ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Second y-axis for rolling correlation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(time_idx[window:], r.iloc[window:], label=r_label, alpha=0.8, color='purple')\n",
    "ax2.set_ylabel(\"Rolling Correlation (r)\")\n",
    "ax2.set_ylim(-1, 1)  # Force correlation scale\n",
    "\n",
    "fig.suptitle(title)\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309c4df",
   "metadata": {},
   "source": [
    "We run the four detectors on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity define data explicitly again.\n",
    "\n",
    "X = cases_df['Los Angeles']\n",
    "Y = cases_df['San Francisco']\n",
    "window = 60\n",
    "S = rolling_corr_test(X=X, Y=Y, window=window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf06889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSUM \n",
    "\n",
    "burn_in = 90; h = 0.7; k = 0.45\n",
    "\n",
    "mu0_est = S.iloc[window:window+burn_in].mean()\n",
    "data = S.iloc[window+burn_in:]\n",
    "\n",
    "cusum_tau = detect_cusum(data, mu0_est, k, h)\n",
    "cusum_tau += window + burn_in\n",
    "cusum_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ecc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon Ranksum\n",
    "\n",
    "w_min = 85; H = 12.4\n",
    "\n",
    "data = S.iloc[window:]\n",
    "\n",
    "ranksum_tau = detect_ranksum(x=data, w_min=w_min, H=H)[0]\n",
    "ranksum_tau += window\n",
    "ranksum_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd438930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEWMA\n",
    "\n",
    "burn_in = 110; lam = 0.4; H = 13\n",
    "\n",
    "mewma_tau = detect_mewma(x=X, y=Y, burn_in=burn_in, lam=lam, H=H)\n",
    "mewma_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive EWMA\n",
    "\n",
    "burn_in = 90; h = 0.5; eta = 0.01; lam = 0.5\n",
    "\n",
    "S = rolling_corr_test(X=X, Y=Y, window=window)\n",
    "mu0_est = S.iloc[window:window+burn_in].mean()\n",
    "data = S.iloc[window+burn_in:]\n",
    "\n",
    "adaptive_ewma_tau = detect_adaptive_ewma(data, mu0=mu0_est, eta=eta, h=h, lam=lam)\n",
    "adaptive_ewma_tau += window + burn_in\n",
    "adaptive_ewma_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 60\n",
    "r = rolling_corr_test(cases_df['Los Angeles'], cases_df['San Francisco'], window=window)\n",
    "\n",
    "x_label = 'Los Angeles County (per 100k)'\n",
    "y_label = 'San Francisco County (per 100k)'\n",
    "r_label = 'Rolling Correlation (w=60)'\n",
    "\n",
    "time_idx = cases_df.index[window:]  # align with plotted slices\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "ax1.plot(time_idx, cases_df['Los Angeles'].iloc[window:], label=x_label, alpha=0.8)\n",
    "ax1.plot(time_idx, cases_df['San Francisco'].iloc[window:], label=y_label, alpha=0.8)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Daily new cases per 100k\")\n",
    "ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(time_idx, r.iloc[window:], label=r_label, alpha=0.8, color='purple')\n",
    "ax2.set_ylabel(\"Rolling Correlation (r)\")\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "# Vertical lines at detected changepoints (using your existing vars)\n",
    "if cusum_tau is not None:\n",
    "    ax1.axvline(cases_df.index[cusum_tau], color='red', linestyle=':', label='CUSUM τ', linewidth=2, alpha=0.9)\n",
    "if adaptive_ewma_tau is not None:\n",
    "    ax1.axvline(cases_df.index[adaptive_ewma_tau], color='blue', linestyle=':', label='Adaptive EWMA τ', linewidth=2, alpha=0.9)\n",
    "if ranksum_tau is not None:\n",
    "    ax1.axvline(cases_df.index[ranksum_tau], color='green', linestyle=':', label='Wilcoxon Ranksum τ', linewidth=2, alpha=0.9)\n",
    "if mewma_tau is not None:\n",
    "    ax1.axvline(cases_df.index[mewma_tau], color='darkorange', linestyle=':', label='MEWMA τ', linewidth=2, alpha=0.9)\n",
    "\n",
    "fig.suptitle(\"LA vs SF COVID-19 Per-Capita Daily New Cases Over Time\", fontsize=18, fontweight=\"bold\")\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1 + h2, l1 + l2, loc='center left', fontsize=11.5,\n",
    "           bbox_to_anchor=(0, 0.84))  # (x, y) shift\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = {\n",
    "    \"A-EWMA\": adaptive_ewma_tau,\n",
    "    \"CUSUM\": cusum_tau,\n",
    "    \"Wilcoxon\": ranksum_tau,\n",
    "    \"MEWMA\": mewma_tau\n",
    "}\n",
    "\n",
    "time_idx = cases_df.index\n",
    "r = rolling_corr_test(cases_df['Los Angeles'], cases_df['San Francisco'], window=window)\n",
    "\n",
    "summary = []\n",
    "for method, t in taus.items():\n",
    "    if t is not None and 0 <= t < len(time_idx):\n",
    "        summary.append((method, t, time_idx[t].date(), float(r.iloc[t])))\n",
    "\n",
    "summary.sort(key=lambda row: row[1])\n",
    "\n",
    "for row in summary:\n",
    "    print(f\"{row[0]:9s} t={row[1]:4d} date={row[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002bf748",
   "metadata": {},
   "source": [
    "## Case Study 3: Zoom vs S&P500\n",
    "\n",
    "Finally, we look at Zoom vs. S&P 500 returns between April 2019 and December 2020. \n",
    "\n",
    "(Source for Zoom dataset: https://www.kaggle.com/datasets/ranugadisansagamage/zoom-stocks;\n",
    "\n",
    "source for S&P 500 dataset: https://www.kaggle.com/datasets/ancyneuray/sp500-2005-2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2464ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "\n",
    "# Zoom 2019-04-18 to 2022-05-20\n",
    "dataset_zoom = kagglehub.dataset_download(\"ranugadisansagamage/zoom-stocks\")\n",
    "X_df = pd.read_csv(dataset_zoom + \"/Zoom.csv\")\n",
    "\n",
    "# S&P 500 July 2005 to June 2025\n",
    "dataset_sp500 = kagglehub.dataset_download(\"ancyneuray/sp500-2005-2025\")\n",
    "Y_df = pd.read_csv(dataset_sp500 + \"/SP500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format date & join streams into one df.\n",
    "\n",
    "start_date = '2019-04-18'\n",
    "end_date = '2020-12-31'\n",
    "X_df['Date'] = pd.to_datetime(X_df['Date'])\n",
    "X_df = X_df[(X_df['Date'] >= start_date) & (X_df['Date'] <= end_date)]\n",
    "Y_df['Date'] = pd.to_datetime(Y_df['Date'], format='%d-%m-%Y')\n",
    "Y_df = Y_df[(Y_df['Date'] >= start_date) & (Y_df['Date'] <= end_date)]\n",
    "\n",
    "combined_df = pd.merge(X_df[['Date','Close']], \n",
    "                       Y_df[['Date','Close']], \n",
    "                       on='Date', how='inner', \n",
    "                       suffixes=('_ZM', '_SPX'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns & performance (for visualization)\n",
    "\n",
    "combined_df['Returns_ZM'] = combined_df['Close_ZM'].pct_change()\n",
    "combined_df['Returns_SPX'] = combined_df['Close_SPX'].pct_change()\n",
    "combined_df['Perf_ZM'] = (1 + combined_df['Returns_ZM']).cumprod().fillna(1)\n",
    "combined_df['Perf_SPX'] = (1 + combined_df['Returns_SPX']).cumprod().fillna(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4b9e7",
   "metadata": {},
   "source": [
    "The correlation is calculated based on each stock's returns.\n",
    "\n",
    "For visualization only, we plot the **performance of each stock** (i.e. how much $1 on day 1 would result on the subsequent days) along with the correlation of the **returns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffe940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the series with r.\n",
    "\n",
    "x_label = \"Zoom Performance\"\n",
    "y_label = \"S&P 500 Performance\"\n",
    "\n",
    "window = 60\n",
    "r = combined_df['Returns_ZM'].rolling(window).corr(combined_df['Returns_SPX'])\n",
    "r_label = 'Rolling Correlation (w=60)'\n",
    "\n",
    "# First y-axis for performance curves\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.plot(combined_df['Date'][window:], combined_df['Perf_ZM'][window:], label=x_label, alpha=0.8)\n",
    "ax1.plot(combined_df['Date'][window:], combined_df['Perf_SPX'][window:], label=y_label, alpha=0.8)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Performance (Normalized)\")\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Second y-axis for rolling correlation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(combined_df['Date'][window:], r[window:], label=r_label, alpha=0.8, color='purple')\n",
    "ax2.set_ylabel(\"Rolling Correlation (r)\")\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "fig.suptitle(\"Zoom vs S&P 500 — Performance & Rolling Correlation of Daily Returns\")\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852d1ab",
   "metadata": {},
   "source": [
    "Next, we apply the four detectors to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3353a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity define data explicitly again.\n",
    "\n",
    "X = combined_df['Returns_ZM']\n",
    "Y = combined_df['Returns_SPX']\n",
    "window = 60\n",
    "S = rolling_corr_test(X=X, Y=Y, window=window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ad274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSUM \n",
    "\n",
    "burn_in = 90; h = 0.7; k = 0.45\n",
    "\n",
    "mu0_est = S.iloc[window:window+burn_in].mean()\n",
    "data = S.iloc[window+burn_in:]\n",
    "\n",
    "cusum_tau = detect_cusum(data, mu0_est, k, h)\n",
    "cusum_tau += window + burn_in\n",
    "cusum_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38103146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon Ranksum\n",
    "\n",
    "w_min = 85; H = 12.4\n",
    "\n",
    "data = S.iloc[window:]\n",
    "\n",
    "ranksum_tau = detect_ranksum(x=data, w_min=w_min, H=H)[0]\n",
    "ranksum_tau += window\n",
    "ranksum_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEWMA\n",
    "\n",
    "burn_in = 110; lam = 0.4; H = 13\n",
    "\n",
    "mewma_tau = detect_mewma(x=X, y=Y, burn_in=burn_in, lam=lam, H=H)\n",
    "print(mewma_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive EWMA\n",
    "\n",
    "burn_in = 90; h = 0.5; eta = 0.01; lam = 0.5\n",
    "\n",
    "mu0_est = S.iloc[window:window+burn_in].mean()\n",
    "data = S.iloc[window+burn_in:]\n",
    "\n",
    "adaptive_ewma_tau = detect_adaptive_ewma(data, mu0=mu0_est, eta=eta, h=h, lam=lam)\n",
    "adaptive_ewma_tau += window + burn_in\n",
    "adaptive_ewma_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results.\n",
    "\n",
    "time_idx = combined_df['Date'].iloc[window:]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "ax1.plot(time_idx, combined_df['Perf_ZM'].iloc[window:], label=x_label, alpha=0.8)\n",
    "ax1.plot(time_idx, combined_df['Perf_SPX'].iloc[window:], label=y_label, alpha=0.8)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Performance (Normalized to 1)\")\n",
    "ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(time_idx, r.iloc[window:], label=r_label, alpha=0.8, color='purple')\n",
    "ax2.set_ylabel(\"Rolling Correlation (r)\")\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "# Vertical lines at detected changepoints\n",
    "if cusum_tau is not None:\n",
    "    ax1.axvline(combined_df['Date'].iloc[cusum_tau], \n",
    "                color='red', linestyle=':', label='CUSUM τ', linewidth=2, alpha=0.9)\n",
    "if adaptive_ewma_tau is not None:\n",
    "    ax1.axvline(combined_df['Date'].iloc[adaptive_ewma_tau], \n",
    "                color='blue', linestyle=':', label='Adaptive EWMA τ', linewidth=2, alpha=0.9)\n",
    "if ranksum_tau is not None:\n",
    "    ax1.axvline(combined_df['Date'].iloc[ranksum_tau], \n",
    "                color='green', linestyle=':', label='Wilcoxon Ranksum τ', linewidth=2, alpha=0.9)\n",
    "if mewma_tau is not None:\n",
    "    ax1.axvline(combined_df['Date'].iloc[mewma_tau], \n",
    "                color='darkorange', linestyle=':', label='MEWMA τ', linewidth=2, alpha=0.9)\n",
    "\n",
    "fig.suptitle(\"Zoom vs S&P 500 — Performance & Rolling Correlation of Daily Returns\", \n",
    "             fontsize=18, fontweight=\"bold\")\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1 + h2, l1 + l2, loc='center left', fontsize=11.5,\n",
    "           bbox_to_anchor=(0, 0.87))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28692846",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = {\n",
    "    \"A-EWMA\": adaptive_ewma_tau,\n",
    "    \"CUSUM\": cusum_tau,\n",
    "    \"Wilcoxon\": ranksum_tau,\n",
    "    \"MEWMA\": mewma_tau,\n",
    "}\n",
    "\n",
    "combined_df = combined_df.sort_values('Date').reset_index(drop=True)\n",
    "time_idx = pd.to_datetime(combined_df['Date'])\n",
    "r = combined_df['Returns_ZM'].rolling(window).corr(combined_df['Returns_SPX'])\n",
    "\n",
    "summary = []\n",
    "for method, t in taus.items():\n",
    "    if t is not None and 0 <= t < len(time_idx):\n",
    "        summary.append((method, t, time_idx.iloc[t].date(), float(r.iloc[t])))\n",
    "\n",
    "summary.sort(key=lambda row: row[1])\n",
    "\n",
    "for row in summary:\n",
    "    print(f\"{row[0]:9s} t={row[1]:4d} date={row[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
